furqn@legion:~/Desktop/Dersler/Comp430/ModelProject$ python3 model_train_classifier.py 
**************************************************
Training Classifier
WARNING: EMBER feature version 2 were computed using lief version 0.9.0-
WARNING:   lief version 0.12.3-39115d10 found instead. There may be slight inconsistencies
WARNING:   in the feature calculations.
DISCLAIMER: Extracting malware only data, which might take a minute...
Running Classifier on: <data_classes.EmberDataset object at 0x7f1d90acbfd0> with batch-size:128, Using-device: cuda
Epoch: 0 | loss: 279.5377477616754
Epoch: 10 | loss: 175.58397625160842
Epoch: 20 | loss: 159.41949451502694
Epoch: 30 | loss: 144.94295849196914
Epoch: 40 | loss: 135.36970245760165
Epoch: 50 | loss: 126.55239273547606
Epoch: 60 | loss: 123.06580568516918
Epoch: 70 | loss: 119.02139860222618
Epoch: 80 | loss: 116.49460260279176
Epoch: 90 | loss: 112.77719396365768
Epoch: 100 | loss: 109.44700016762204
Epoch: 110 | loss: 108.39686408447206
Epoch: 120 | loss: 107.10538936480312
Epoch: 130 | loss: 106.26549354543343
Epoch: 140 | loss: 104.26998662539111
Epoch: 150 | loss: 104.6848532810336
Epoch: 160 | loss: 102.59383417371835
Epoch: 170 | loss: 100.50519666878452
Epoch: 180 | loss: 100.08373148822213
Epoch: 190 | loss: 100.56039493622801
Epoch: 200 | loss: 97.73519088748758
Epoch: 210 | loss: 98.1937287749485
Epoch: 220 | loss: 99.71059314537725
Epoch: 230 | loss: 97.82747724031545
Epoch: 240 | loss: 95.77042523734963
Epoch: 250 | loss: 94.95844490883005
Epoch: 260 | loss: 93.88300244719798
Epoch: 270 | loss: 93.36284403166569
Epoch: 280 | loss: 92.95130267742546
Epoch: 290 | loss: 93.0699578517136
Epoch: 300 | loss: 91.94101844775118
Epoch: 310 | loss: 91.96122862130501
Epoch: 320 | loss: 92.40243373377372
Epoch: 330 | loss: 90.88751228756577
Epoch: 340 | loss: 90.17248841713923
Epoch: 350 | loss: 91.04931280592146
Epoch: 360 | loss: 90.04567729659793
Epoch: 370 | loss: 88.93162281634113
Epoch: 380 | loss: 90.01597340364363
Epoch: 390 | loss: 88.26283604706127
Epoch: 400 | loss: 88.44929225351194
Epoch: 410 | loss: 87.59210580359888
Epoch: 420 | loss: 87.85000061546833
Epoch: 430 | loss: 87.51852000149671
Epoch: 440 | loss: 86.67954304843589
Epoch: 450 | loss: 87.07967649629357
Epoch: 460 | loss: 85.80835717487491
Epoch: 470 | loss: 86.71431732691231
Epoch: 480 | loss: 85.57513525740791
Epoch: 490 | loss: 85.7952046909795
Epoch: 500 | loss: 86.10915191209953
Epoch: 510 | loss: 84.86407903821544
Epoch: 520 | loss: 85.20968296483663
Epoch: 530 | loss: 84.76041213080953
Epoch: 540 | loss: 84.29196724532865
Epoch: 550 | loss: 84.50747285576917
Epoch: 560 | loss: 83.78133781291666
Epoch: 570 | loss: 84.22091606106596
Epoch: 580 | loss: 84.92093851392116
Epoch: 590 | loss: 84.02743520477996
Epoch: 600 | loss: 84.16488285656844
Epoch: 610 | loss: 83.8011138519128
Epoch: 620 | loss: 82.77549817530904
Epoch: 630 | loss: 85.6545227780987
Epoch: 640 | loss: 84.58638802063504
Epoch: 650 | loss: 83.68814861319447
Epoch: 660 | loss: 83.33075450186901
Epoch: 670 | loss: 83.8282447050416
Epoch: 680 | loss: 82.79641492097693
Epoch: 690 | loss: 82.2255255139901
Epoch: 700 | loss: 82.4358182976849
Epoch: 710 | loss: 82.0551194708181
Epoch: 720 | loss: 82.0809104191698
Epoch: 730 | loss: 82.2804094346295
Epoch: 740 | loss: 81.95347224561053
Epoch: 750 | loss: 81.80492782586273
Epoch: 760 | loss: 81.84376531761372
Epoch: 770 | loss: 81.44749612565128
Epoch: 780 | loss: 81.31159754570334
Epoch: 790 | loss: 82.01757090419304
Epoch: 800 | loss: 82.68365415154003
Epoch: 810 | loss: 82.33320339188191
Epoch: 820 | loss: 81.20114079010526
Epoch: 830 | loss: 81.02394896258184
Epoch: 840 | loss: 80.77182128268214
Epoch: 850 | loss: 81.0231744489077
Epoch: 860 | loss: 80.075233234314
Epoch: 870 | loss: 81.06005244992161
Epoch: 880 | loss: 80.3956799360242
Epoch: 890 | loss: 81.14935559164347
Epoch: 900 | loss: 79.97268845821804
Epoch: 910 | loss: 79.83753464119906
Epoch: 920 | loss: 80.1040314077486
Epoch: 930 | loss: 79.58430711172659
Epoch: 940 | loss: 79.10343388522465
Epoch: 950 | loss: 79.49577874120735
Epoch: 960 | loss: 78.55410532628957
Epoch: 970 | loss: 78.58192081448816
Epoch: 980 | loss: 78.58332115996755
Epoch: 990 | loss: 78.02156165121946
Epoch: 1000 | loss: 78.0833182996596
Epoch: 1010 | loss: 78.14868778409588
Epoch: 1020 | loss: 78.28532122655679
Epoch: 1030 | loss: 77.73672874390493
Epoch: 1040 | loss: 77.6093116244027
Epoch: 1050 | loss: 77.76051589544363
Epoch: 1060 | loss: 78.56877538558571
Epoch: 1070 | loss: 77.18836900530101
Epoch: 1080 | loss: 77.0229112470813
Epoch: 1090 | loss: 77.51987004455589
Epoch: 1100 | loss: 77.32291885921194
Epoch: 1110 | loss: 76.86588318709329
Epoch: 1120 | loss: 76.4981553533735
Epoch: 1130 | loss: 80.55820592760129
Epoch: 1140 | loss: 76.84818744919423
Epoch: 1150 | loss: 76.9627851006639
Epoch: 1160 | loss: 77.25367764285242
Epoch: 1170 | loss: 76.85043698893769
Epoch: 1180 | loss: 76.38002601698155
Epoch: 1190 | loss: 77.23911601714505
Epoch: 1200 | loss: 76.56021580766306
Epoch: 1210 | loss: 77.40493589675414
Epoch: 1220 | loss: 76.02715244307512
Epoch: 1230 | loss: 76.94017371974049
Epoch: 1240 | loss: 76.29466131020789
Epoch: 1250 | loss: 76.11904700644794
Epoch: 1260 | loss: 77.33890840268187
Epoch: 1270 | loss: 75.87821527551799
Epoch: 1280 | loss: 75.86131136757987
Epoch: 1290 | loss: 76.80181964039153
Epoch: 1300 | loss: 76.65269800250377
Epoch: 1310 | loss: 76.48359038252607
Epoch: 1320 | loss: 76.7272842851434
Epoch: 1330 | loss: 76.60978863237989
Epoch: 1340 | loss: 75.86994848536041
Epoch: 1350 | loss: 75.78696365405845
Epoch: 1360 | loss: 74.62162992634455
Epoch: 1370 | loss: 74.87541184771801
Epoch: 1380 | loss: 75.92657820597665
Epoch: 1390 | loss: 75.98570946479701
Epoch: 1400 | loss: 75.8536237525706
Epoch: 1410 | loss: 76.24409964534422
Epoch: 1420 | loss: 75.61823513229645
Epoch: 1430 | loss: 75.47734285986931
Epoch: 1440 | loss: 74.94960234163892
Epoch: 1450 | loss: 74.661118046668
Epoch: 1460 | loss: 75.89121503515358
Epoch: 1470 | loss: 74.97382101835835
Epoch: 1480 | loss: 74.52881718708879
Epoch: 1490 | loss: 74.5368436580136
Epoch: 1500 | loss: 74.56609125498727
Epoch: 1510 | loss: 74.30073971160756
Epoch: 1520 | loss: 75.06043678983607
Epoch: 1530 | loss: 73.63615572972542
Epoch: 1540 | loss: 74.84878625734552
Epoch: 1550 | loss: 74.319936136253
Epoch: 1560 | loss: 73.95168449977493
Epoch: 1570 | loss: 74.46361017253173
Epoch: 1580 | loss: 74.42596348187395
Epoch: 1590 | loss: 74.17319162643332
Epoch: 1600 | loss: 73.81682444645769
Epoch: 1610 | loss: 73.72407556824882
Epoch: 1620 | loss: 75.11509573622125
Epoch: 1630 | loss: 73.8300061072492
Epoch: 1640 | loss: 73.12169227481799
Epoch: 1650 | loss: 73.47285180048958
Epoch: 1660 | loss: 73.08546929420535
Epoch: 1670 | loss: 73.03862481678846
Epoch: 1680 | loss: 74.06028556222483
Epoch: 1690 | loss: 72.97216918911772
Epoch: 1700 | loss: 72.92121652753428
Epoch: 1710 | loss: 72.9938482271026
Epoch: 1720 | loss: 72.56802417225771
Epoch: 1730 | loss: 73.21876163759565
Epoch: 1740 | loss: 73.21121260286547
Epoch: 1750 | loss: 73.56642318720905
Epoch: 1760 | loss: 73.14216919868957
Epoch: 1770 | loss: 72.68802878913858
Epoch: 1780 | loss: 72.72265563263367
Epoch: 1790 | loss: 72.41766553314457
Epoch: 1800 | loss: 72.70190501343073
Epoch: 1810 | loss: 72.65974984232706
Epoch: 1820 | loss: 72.2082296701528
Epoch: 1830 | loss: 72.89405376239067
Epoch: 1840 | loss: 72.28771920750809
Epoch: 1850 | loss: 72.22431740382505
Epoch: 1860 | loss: 71.87147550013489
Epoch: 1870 | loss: 71.81945032534578
Epoch: 1880 | loss: 72.06122969489061
Epoch: 1890 | loss: 72.53996282009938
Epoch: 1900 | loss: 72.60380598281957
Epoch: 1910 | loss: 72.0408629665848
Epoch: 1920 | loss: 71.6299481856004
Epoch: 1930 | loss: 72.61986077540314
Epoch: 1940 | loss: 72.3764622944652
Epoch: 1950 | loss: 72.00896757431613
Epoch: 1960 | loss: 71.81447998400994
Epoch: 1970 | loss: 71.67750362501019
Epoch: 1980 | loss: 72.22379064507936
Epoch: 1990 | loss: 72.09508979860023
Epoch: 2000 | loss: 72.09254114818937
Epoch: 2010 | loss: 71.76911088606523
Epoch: 2020 | loss: 71.98575053259226
Epoch: 2030 | loss: 71.72817722551176
Epoch: 2040 | loss: 71.6369280391977
Epoch: 2050 | loss: 72.82370795671916
Epoch: 2060 | loss: 72.22898763986944
Epoch: 2070 | loss: 72.53171622681644
Epoch: 2080 | loss: 71.92727456029785
Epoch: 2090 | loss: 71.97333978397636
Epoch: 2100 | loss: 72.44446532442889
Epoch: 2110 | loss: 71.91855256976062
Epoch: 2120 | loss: 73.25556623240203
Epoch: 2130 | loss: 71.83890062135075
Epoch: 2140 | loss: 70.9805270593325
Epoch: 2150 | loss: 71.5862307705822
Epoch: 2160 | loss: 72.11042156292282
Epoch: 2170 | loss: 71.91299470417373
Epoch: 2180 | loss: 71.31958452903848
Epoch: 2190 | loss: 71.01414677237476
Epoch: 2200 | loss: 71.35872865945198
Epoch: 2210 | loss: 71.21299036963555
Epoch: 2220 | loss: 71.05109623649778
Epoch: 2230 | loss: 71.40050610474178
Epoch: 2240 | loss: 70.83373295838337
Epoch: 2250 | loss: 71.29868744958839
Epoch: 2260 | loss: 71.4188394171786
Epoch: 2270 | loss: 71.07398089951751
Epoch: 2280 | loss: 71.22376099449858
Epoch: 2290 | loss: 70.45891078919855
Epoch: 2300 | loss: 71.71584438972151
Epoch: 2310 | loss: 70.93464247740992
Epoch: 2320 | loss: 71.09888203394972
Epoch: 2330 | loss: 70.74526267899292
Epoch: 2340 | loss: 71.1580037336833
Epoch: 2350 | loss: 70.40084092060638
Epoch: 2360 | loss: 70.59610342888432
Epoch: 2370 | loss: 70.88666581373049
Epoch: 2380 | loss: 71.623393368435
Epoch: 2390 | loss: 70.56447246517973
Epoch: 2400 | loss: 69.84824446529443
Epoch: 2410 | loss: 70.54721194284693
Epoch: 2420 | loss: 70.77161550944348
Epoch: 2430 | loss: 69.65790838666155
Epoch: 2440 | loss: 70.4519050699804
Epoch: 2450 | loss: 70.40769784056633
Epoch: 2460 | loss: 69.71233375788125
Epoch: 2470 | loss: 70.06238760773982
Epoch: 2480 | loss: 69.87157744650233
Epoch: 2490 | loss: 70.40581937496638
Epoch: 2500 | loss: 69.02476889486532
Epoch: 2510 | loss: 69.48017859530422
Epoch: 2520 | loss: 70.33886330416314
Epoch: 2530 | loss: 69.83072607305344
Epoch: 2540 | loss: 69.67044632502444
Epoch: 2550 | loss: 69.7854120866553
Epoch: 2560 | loss: 69.3349234855032
Epoch: 2570 | loss: 69.03883436845459
Epoch: 2580 | loss: 69.89861390790867
Epoch: 2590 | loss: 69.720382638105
Epoch: 2600 | loss: 69.88753569139129
Epoch: 2610 | loss: 69.56458194383228
Epoch: 2620 | loss: 69.04586351853811
Epoch: 2630 | loss: 68.95469756162804
Epoch: 2640 | loss: 69.23128944108895
Epoch: 2650 | loss: 69.61178389116878
Epoch: 2660 | loss: 69.7550041429869
Epoch: 2670 | loss: 69.328414278436
Epoch: 2680 | loss: 69.47353416323791
Epoch: 2690 | loss: 69.81637680907546
Epoch: 2700 | loss: 68.86136012898926
Epoch: 2710 | loss: 69.24763221621123
Epoch: 2720 | loss: 69.65088846600302
Epoch: 2730 | loss: 69.22668635825684
Epoch: 2740 | loss: 70.11367832866854
Epoch: 2750 | loss: 69.19719835924089
Epoch: 2760 | loss: 68.94695130585325
Epoch: 2770 | loss: 69.1003499861112
Epoch: 2780 | loss: 69.16118134179578
Epoch: 2790 | loss: 69.0673658981058
Epoch: 2800 | loss: 69.1293608706806
Epoch: 2810 | loss: 69.14577112762072
Epoch: 2820 | loss: 69.11429817088948
Epoch: 2830 | loss: 68.91480741168057
Epoch: 2840 | loss: 68.43752634167801
Epoch: 2850 | loss: 68.97209000021967
Epoch: 2860 | loss: 69.21329479922125
Epoch: 2870 | loss: 69.15383298439964
Epoch: 2880 | loss: 69.07841540253929
Epoch: 2890 | loss: 68.88977084802048
Epoch: 2900 | loss: 69.01104340116487
Epoch: 2910 | loss: 68.50680657061002
Epoch: 2920 | loss: 68.95858469793944
Epoch: 2930 | loss: 68.78930716478187
Epoch: 2940 | loss: 68.43868271839138
Epoch: 2950 | loss: 68.53300557726732
Epoch: 2960 | loss: 69.25880872442782
Epoch: 2970 | loss: 68.58782343633042
Epoch: 2980 | loss: 68.9064444567411
Epoch: 2990 | loss: 69.08713242869861
Epoch: 3000 | loss: 68.09058562067369
Epoch: 3010 | loss: 68.23190381352748
Epoch: 3020 | loss: 68.17805656722484
Epoch: 3030 | loss: 68.04576204913783
Epoch: 3040 | loss: 68.13900146580575
Epoch: 3050 | loss: 68.57494994208233
Epoch: 3060 | loss: 67.99230162774984
Epoch: 3070 | loss: 68.0984583904595
Epoch: 3080 | loss: 67.74461305277948
Epoch: 3090 | loss: 67.65901993349958
Epoch: 3100 | loss: 67.8627724300424
Epoch: 3110 | loss: 68.40820126646608
Epoch: 3120 | loss: 68.26052218116617
Epoch: 3130 | loss: 67.80237505207145
Epoch: 3140 | loss: 67.71535978998456
Epoch: 3150 | loss: 67.6858514393949
Epoch: 3160 | loss: 68.90193786680894
Epoch: 3170 | loss: 67.82551156675022
Epoch: 3180 | loss: 68.00183299073346
Epoch: 3190 | loss: 67.68195227136269
Epoch: 3200 | loss: 68.66935710597585
Epoch: 3210 | loss: 68.54237683431013
Epoch: 3220 | loss: 67.99689183305368
Epoch: 3230 | loss: 68.13395054903259
Epoch: 3240 | loss: 67.96546896307393
Epoch: 3250 | loss: 67.4052544927129
Epoch: 3260 | loss: 67.3375292615061
Epoch: 3270 | loss: 68.07104948208533
Epoch: 3280 | loss: 67.16332609871871
Epoch: 3290 | loss: 67.62890350428637
Epoch: 3300 | loss: 69.02559533575499
Epoch: 3310 | loss: 68.56733919320996
Epoch: 3320 | loss: 67.62673390857395
Epoch: 3330 | loss: 68.2678931245627
Epoch: 3340 | loss: 67.77784297445479
Epoch: 3350 | loss: 69.42618377336109
Epoch: 3360 | loss: 68.24392626329752
Epoch: 3370 | loss: 67.54608703452602
Epoch: 3380 | loss: 67.84083012055154
Epoch: 3390 | loss: 67.7287296797093
Epoch: 3400 | loss: 67.5480414713482
Epoch: 3410 | loss: 67.37900114208948
Epoch: 3420 | loss: 67.1351973246397
Epoch: 3430 | loss: 67.63189061049417
Epoch: 3440 | loss: 66.87385180355288
Epoch: 3450 | loss: 66.66648191434606
Epoch: 3460 | loss: 67.02740514824409
Epoch: 3470 | loss: 67.91446855498505
Epoch: 3480 | loss: 66.73268798802906
Epoch: 3490 | loss: 67.50559626007808


WARNING: EMBER feature version 2 were computed using lief version 0.9.0-
WARNING:   lief version 0.12.3-39115d10 found instead. There may be slight inconsistencies
WARNING:   in the feature calculations.
**************************************************
Testing Identifier
/home/furqn/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:172: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)
  return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)
Test accuracy:  0.354415
WARNING: EMBER feature version 2 were computed using lief version 0.9.0-
WARNING:   lief version 0.12.3-39115d10 found instead. There may be slight inconsistencies
WARNING:   in the feature calculations.
DISCLAIMER: Extracting malware only data, which might take a minute...
**************************************************
Testing Classifier
Test accuracy:  0.011620040590552747
**************************************************
Testing Isolation Forest
Test accuracy:  0.995115
**************************************************
Testing Pipeline
WARNING: EMBER feature version 2 were computed using lief version 0.9.0-
WARNING:   lief version 0.12.3-39115d10 found instead. There may be slight inconsistencies
WARNING:   in the feature calculations.
Test accuracy:  0.00051